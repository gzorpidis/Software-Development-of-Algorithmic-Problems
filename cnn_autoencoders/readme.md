# Ανάπτυξη Λογισμικού για δυσεπίλυτα Αλγοριθμικά Προβλήματα
## Εργασία 3

##### Εκπόνηση από: 

1) Ζορπίδης Γεώργιος(1115202000055)

2) Κωνσταντίνος Φράγκος (1115202000207)

---
### Λίγα πράγματα για την υλοποίηση

Οι αλγόριθμοι που υλοποιούμε σε αυτή την εργασία, είναι οι ίδιοι με την 1η κα την 2η εργασία (K-nearest-neigbours,clustering), απλά στη συγκεκριμένη εργασία τα input είναι διαφορετικά. Στις προηγούμενες εργασίες δεχόμασταν σαν input file, δύο αρχεία από το mnist dataset, ένα για dataset και ένα για query's, τα οποία περιήχαν εικόνες μεγέθους 28*28. Σε αυτή την εργασία, τα dataset μας περιέχουν encoded εικόνες 10 και 30 διαστάσεων, με σκοπό τη βελτίωση της αποδοτικότητας σε αντίστροφη σχέση με την αποτελεσματικότητα.

Τα αρχικά projects έχουν επεκταθεί ως προς το εξής: Ένα flag `-latent` έχει προστεθεί για τον LSH και Hypercube, ο οποίος μπορεί είτε να μην υπάρχει καθόλου είτε να έχει τιμές No ή Yes. Αν το latent flag είναι αληθές, αυτό σημαίνει ότι τα δεδομένα εντός του dataset είναι σε latent dimension και θα πρέπει να δωθούν επίσης τα flags: `-id` και `-iq` για το αρχικό dataset και το query set αντίστοιχα (στο αρχικό/initial space) έτσι ώστε να υπάρχει αντιστοίχιση με βάση το αρχικό dataset. Αυτό προϋποθέτει ορθή κατασκεύη των dataset ώστε να έχουν σωστή αντιστοιχία. Τα αποτελέσματα τότε (αν latent=true) εκτυπώνονται με βάσει τον αρχικό χώρο (και προστίθεται και ο υπολογισμός του Approximation Factor μετά τους χρπνους. Αν δεν δωθεί, τότε κανονικά έχουμε υπολογισμό των αποτελεσμάτων στον αρχικό χώρο.

Επίσης έχουμε αλλαγή και στο clustering. Για τον υπολογισμό της silhouette πλέον έχουμε 2 επιλογές, την `_compute_latent_silhouettes()` και την `_compute_silhouettes()`. Η 1η παίρνει ως Input το αρχικό dataset για τον υπολογισμό του dataset. Ξανά, χρειάζεται σωστή αντιστοιχιία latent και initial space datasets.

### Υπολογισμός αποτελεσμάτων
Στην εργασία αυτή, βρίσκουμε τον κοντινότερο γείτονα με τον ίδιο τρόπο όπως στις προηγούμενες εργασίες αλλά σε χαμηλότερη διάσταση, και στο τέλος αντιστοιχούμε τον γείτονα και την εικόνα του ερωτήματος με τις αντίστοιχες εικόνες 28*28 διαστάεων, και με βάση αυτές υπολογίζουμε τα average aproximator factors και max aproximator factors. Στην lsh και τον υπερκύβο αυτό γίνεται με μεταφορά στην python ενώ στον gnn και mrng αυτό γίνεται απευθείας στην C++.


## Μέρος Α

Για να βγάλουμε το καλύτερο μοντέλο, πειραματιστήκαμε με διάφορες τιμές για διάφορες παραμέτρους. Αρχικά παίξαμε με τον αριθμό των layers, από 2 εώς 3, τον αριθμό των epochs (20,100), τα νούμερα των εκάστοτε layers (από χαμηλά όπως 8,16 έως πιο πάνω μέχρι 128,256). Το batch size παρέμεινε απαράλλαχτο στα 128 αφού θεωρήσαμε ότι η μεταβολή των άλλων παραμέτρων ήταν ήδη αρκετή για να βγάλουμε κάποια αποτελέσματα.

Απο κάτω είναι κάποια ενδεικτικά αποτελέσματα των 10 πρώτων test images original (πρώτη γραμμή) και των εκάστοτε reconstructed (δεύτερη γραμμή).

Βλέπουμε ότι κάποια μοντέλα με χαμηλό validation loss (0.09, 1η εικόνα) κάνουν reconstruct πολύ καλά τις αρχικές εικόνες, ενώ κάποια άλλα δυσκολεύονται περισσότερο (3η εικόνα) ή και πάρα πολύ (εικόνες 2,4)

![](https://i.ibb.co/HDw5zwF/digits-30.png)

![](https://i.ibb.co/Zc71N3q/digits-8-16.png)

![](https://i.ibb.co/pwdQVqJ/digits-20-64-128.png)

![](https://i.ibb.co/tJYjCxY/digits-20-16-32.png)

Το optuna είναι ένα μοντέλο το οποίο θα μπορούσε να χρησιμοποιηθεί για την περαιτέρω βελτιστοποίηση διάφορων υπερπαραμέτρων. Ωστόσο, λόγω του χρονοκοστοβόρου training περιοριστήκαμε σε manual αλλαγές των παραμέτρων.

#### Δομή

Στα δυο διαφορετικά directories (`lsh_hc_clustering`, `mrng_gnn`) υπάρχουν τα δύο πρώτα projects με μερικές αλλαγές. Για το καθένα ισχύουν ότι:


Για τους GNN/MRNG όπως και παλιά:

Στο directory `includes` υπάρχουν όλα τα header files που χρησιμοποιούνται.

 - `lsh_includes` υπάρχουν τα Includes για το LSH και τον Υπερκύβο για χρήση εντός των modules
 - `graph.hpp` περιέχει τον ορισμό και τον κώδικα για τον γράφο μας, ο οποίος χρησιμοποιεί templates για να είναι generic.
 - `graph_search.hpp` περιέχει την υλοποιήση για τους αλγορίθμους GNN και MRNG μέσω του Class Graph_Search_Ann.

 
Στο directory `modules/gnn` υπάρχουν εσωτερικά όλες οι υλοποιήσεις των modules.

Στο directory `obj` τοποθετούνται τα Object files.

Στο directory `outputs` τοποθετούνται by default αρχεία εξόδου.

Στο directory `data` τοποθετούνται τα αρχεία που διαβάζονται απτον χρήστη (αν θέλει) ή τα άλλα δεδομένα.

Στο directory `lib` υπάρχει η compiled σε library version του LSH/HYPERCUBE όπως παραδόθηκε για την Εργασία 1.


Για τον LSH/Hypercube/Clustering:

Στο directory includes υπάρχουν όλα τα header files που χρησιμοποιούνται.

Cluster.hpp περιέχει τις κλάσεις του clustering
common.hpp περιέχει συχνά χρησιμοποιούμενο κώδικα
get_long_opts_single.hpp μια βιβλιοθήκη που αναπτύξαμε για να διαβάζει τα options απτην γραμμή εντολών
headers.hpp Ένα συνολικό header file αρχείο, που περιέχει πολλά includes, για να μην γίνονται συνέχεια
hypercube.hpp περιέχει τις κλάσεις για τον υπερκύβο
lsh.hpp περιέχει τις κλάσεις για το LSH
structures.hpp περιέχει συχνά χρησιμοποιούμενες κλάσεις, κοινές μεταξύ των υλοποιήσεων
Στο directory modules υπάρχουν εσωτερικά όλες οι υλοποιήσεις των modules και των αλγορίθμων.

Στο directory objects τοποθετούνται τα Object files

Στο directory outputs.txt τοποθετούνται by default αρχεία εξόδου

Στο directory data τοποθετούνται τα αρχεία που διαβάζονται απτον χρήστη (αν θέλει)

---

## Μέρος Β 

### HC

| LATENT DIM | w | k | probes | M | Av. Time (ms) | MAF    | AAF   |
|------------|---|---|--------|---|----------------|--------|-------|
| 10         | 4 | 4 | 2      | 10000 | 8.00       | 10.3616| 3.73  |
| 30         | 4 | 4 | 2      | 10000 | 8.00       | 6.88   | 2.75  |
| 10         | 4 | 8 | 4      | 10000 | 0.81       | 9.31033| 3.57  |
| 30         | 4 | 8 | 4      | 10000 | 0.54       | 9.3272 | 3.01  |
| 10         | 4 | 14| 2      | 10000 | 2.27       | 8.58305| 3.31  |
| 30         | 4 | 14| 2      | 10000 | 2.27       | 9.32126| 3.47  |
| 10         | 500| 4 | 2      | 10000 | 15.81      | 7.84403| 3.27  |
| 30         | 500| 4 | 2      | 10000 | 17.72      | 9.7265 | 3.82  |

### LSH

| LATENT DIM | w  | k  | l | Av. Time (ms) | MAF    | AAF   |
|------------|----|----|---|----------------|--------|-------|
| 10         | 4  | 10 | 8 | 0              | 8.62909| 3.41  |
| 30         | 4  | 10 | 8 | 0.18           | 9.45715| 3.26  |
| 10         | 30 | 10 | 8 | 4.36           | 9.43756| 3.40  |
| 30         | 30 | 10 | 8 | 5.27           | 9.92945| 3.61  |
| 10         | 100| 10 | 8 | 17.90          | 9.97412| 3.72  |
| 30         | 100| 10 | 8 | 17.81          | 7.29959| 2.91  |
| 10         | 1000| 10| 8 | 245.30         | 9.43756| 3.08  |
| 30         | 1000| 10| 8 | 146.09         | 9.7265 | 3.42  |


### GNN

Για Latent Dim=10


### GNN(T=50, E=1)

| R   | Av. Time (ms) | MAF     | AAF     |
| --- | ------------- | ------- | ------- |
| 1   | 0             | 8.3476  | 3.61295 |
| 10  | 0.1           | 8.08656 | 3.5746  |
| 100 | 2.95          | 8.75642 | 3.50653 |

### GNN(T=50, E=30)

| R   | Av. Time (ms) | MAF     | AAF     |
| --- | ------------- | ------- | ------- |
| 1   | 0.05          | 8.73219 | 4.03352 |
| 10  | 4.9           | 9.43756 | 4.68596 |
| 100 | 48.4          | 9.97412 | 4.04597 |

### GNN(T=50, E=50)

| R   | Av. Time (ms) | MAF     | AAF     |
| --- | ------------- | ------- | ------- |
| 1   | 0.238095      | 9.07547 | 3.28836 |
| 10  | 7.33333       | 9.43756 | 4.25721 |
| 100 | 63.85         | 9.97412 | 3.92713 |


Για latent dim=30

### GNN(T=50, E=1)

| R   | Av. Time (ms) | MAF    | AAF    |
| --- | ------------- | ------ | ------ |
| 1   | 0             | 10.7339 | 4.20419 |
| 10  | 0             | 8.43559 | 3.04034 |
| 100 | 3.2           | 10.2127 | 4.64233 |

### GNN(T=50, E=30)

| R   | Av. Time (ms) | MAF    | AAF    |
| --- | ------------- | ------ | ------ |
| 1   | 0.0666667     | 9.71438 | 3.21396 |
| 10  | 6.93333       | 6.88    | 3.04379 |
| 100 | 62            | 3.54102 | 2.38129 |

### GNN(T=50, E=50)

| R   | Av. Time (ms) | MAF    | AAF    |
| --- | ------------- | ------ | ------ |
| 1   | 0.266667      | 9.00698 | 3.9976  |
| 10  | 7.4           | 3.64645 | 2.32333 |
| 100 | 58.2          | 9.7265  | 3.79318 |


### MRNG

Για latent dim=10

| MRNG | l         | Av. Time (ms) | MAF     | AAF     |
| ---- | --------- | ------- | ------- | ------- |
| | 20   | 0 | 9.28136 |  2.84535
| | 50   | 0      | 10.0853 | 3.60193 |         
| | 100  | 0     | 8.81493 | 3.25069 |        
| | 200  | 0   | 10.3616 | 2.85739 |       
| | 500 | 2.5 | 8.81504 | 3.0763
| | 1000 | 11.55   | 8.81504       | 3.09169      |  

Για latent dim=30

| MRNG | l            | Av. Time (ms) | MAF     | AAF     |
| ---- | ------------ | ------------- | ------- | ------- |
|      | 20           | 0             | 7.68882 | 3.31387 |
|      | 50           | 0             | 6.29078 | 2.89048 |
|      | 100          | 0             | 7.64151 | 2.85817 |
|      | 200          | 0             | 7.10863 | 3.34308 |
|      | 500          | 2.73333       | 7.30565 | 2.96108 |
|      | 1000         | 12.0667       | 7.31949 | 3.52193 |



### Συμπεράσματα

Τα παραπάνω στατιστικά παρουσιάζουν ιδιαίτερο ενδιαφέρον. Αρχικά παρατηρούμε ότι όπως αναμέναμε τα δύο καλύτερα Average Approximation Factors (AAF) παρουσιάζονται σε δεδομένα για το μεγαλύτερο latent dimension dataset, των 30. Αυτό το αναμέναμε αφού η λογική μας λέει ότι τα μεγαλύτερα latent dimension datasets κρατούν περισσότερη πληροφορία εντός τους, άρα το decoding θα είναι και καλύτερο (προφανώς ωστόσο εξαρτάται και από το πόσο καλό είναι το μοντέλο μας!). 

Αυτό που αξίζει να σημειωθεί είναι ότι όπως βλέπουμε, καθώς οι παράμετροι αλλάζουν ώστε να μας δίνουν φαινομενικά καλύτερους κοντινούς γείτονες, αυτή είναι πλέον μια πληροφορία η οποία φαίνεται να "βλάπτει"  την αποδοτικότητα του μοντέλου μας. Το ότι στους latent dimension χώρους είμαστε ικανοί να βρούμε τους καλύτερους nearest neighbours, δεν μετραφράζεται και με έναν καλύτερο nearest neighbor στον αρχικό χώρο μας, κατί το οποίο είναι επίσης λογικό. Το compressions/encoding το οποίο δέχονται τα δεδομένα μας, δημιουργεί μια όχι 1-1 αντιστοιχία των καλύτερων γειτόνων του αρχικού σε καλύτερους γείτονες του τελικού, και το αντίστροφο. Άρα, το να επιτρέπουμε στον μειωμένο χώρο, έναν approximate nearest neighbor και όχι τον ακριβή nearest neighbor, δρα προς όφελος μας, δίνοντας μας την δυνατότητα να πέσουμε σε έναν πιο κοντινό αρχικό κοντινό γείτονα. Αυτό το συμπέρασμα βγαίνει τόσο απτούς αλγόριθμους του LSH, Hypercube όσο και για τους γραφοθεωρητικούς αλγόριθμους.

Για τον χρόνο τα αποτελέσματα είναι αναμενόμενα, με τους αλγόριθμους να τρέχουν πάρα πολύ γρήγορα και εννοείται πολλές φορές πιο γρήγορα από τις μεθόδους του brute force για τον αρχικό χώρο. Παρόλα αυτά, οι αλγόριθμοι έχουν πλέον και πολύ γρήγορο brute force finding στις μικρότερες διαστάσεις, άρα το να χρησιμοποιούμε LSH και HC μερικές φορές ίσως να μην είναι το πιο αποδοτικό από πλευράς χρόνου.

Για τον υπολογισμό του Approximation Factor χρησιμοποίησαμε τον τύπο: $\frac{d(p_{ΑΝΑΓ}, q_{ΑΝΑΓ})}{d(p_{ΑΧ},q_{ΑΧ})} $. Η απόσταση $d(p_{ΑΧ},q_{ΑΧ})$ είναι το dTrue το οποίο ξέρουμε, και για τo $d(p_{ΑΝΑΓ}, q_{ΑΝΑΓ})$ και γενικά για να πάμε από το σημείο του latent space στον αρχικό, χρησιμοποίησαμε την 1-1 αντιστοιχία απτό αρχικό και το μειωμένο dataset. Δηλαδή αν για ένα query, ο Nearest Neighbor μέσω LSH (σε μειωμένο χώρο) είναι η εικόνα αριθμού X, τότε αναγάγουμε την εικόνα αυτή διαβάζοντας το αρχικό dataset και παίρνοντας την εικόνα αριθμού X. Για αυτό είναι σημαντικό τα 2 datastes να έχουν δημιουργηθεί με την ίδια σειρά. Αν θέλαμε να αναγάγουμε πραγματικά το query και τον nearest neighbor απτό latent space στον αρχικό, θα χρησιμοποιούσαμε τον trained decoder του μοντέλου μας. Παρόλα αυτά, δεν επιλέξαμε αυτήν την λύση γιατί το μοντέλο μας δέχεται vectors από floats/doubles ενώ εμείς έχουμε pixels (bytes) και η αλλαγή από byte σε float και από float πάλι σε byte χάνει πάρα πολύ πληροφορία με αποτέλεσμα να καθίσταται απαγορευτικό κάτι τέτοιο.


---

## Μέρος Γ

#### Clustering

| Algorithm | Latent Dim | Clustering time | Cluster 1 | Cluster 2 | Cluster 3 | Cluster 4 | Cluster 5 | Cluster 6 | Cluster 7 | Cluster 8 | Cluster 9 | Cluster 10 | Median Silhouette |
|----|---|----|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|------------|-------------------|
| CLASSIC | 784  | 94 seconds | 0.0271513 | 0.0608666 | 0.0826689 | 0.047532  | 0.0531675 | 0.0316848 | 0.104538  | 0.039937  | 0.0419302 | 0.0921239  | 0.0581601         |
| LSH | 784 | 101 seconds | -0.0114186 | 0.147544  | -0.0088162| -0.0174359| -0.00193569| 0.00275603| -0.0124664 | 0.0643903 | 0.0228    | -0.0251353 | 0.0160282         |
| HC | 784 | 84 seconds | 0.0042237  | 0.028284  | -0.0143638| 0.0206095 | -0.0182596 | -0.0180232 | -0.0349175 | -0.0294986 | 0.0507264 | 0.0207376   | 0.000951847       |
| CLASSIC | 10 | 5 seconds | 0.00184074    | 0.00174853    | -0.00202103  | -0.00195479  | -0.000371332 | -0.00155995  | 0.00028467   | 0.000738446  | 0.000128732   | 0.00285841   | 0.000169243      |
| LSH | 10 | 23 seconds | 0.00193809    | -0.000575398  | -0.00153608  | -0.00101068  | -1.76299e-05 | 0.00067664   | -0.00253928  | 0.00231878   | -0.000208136  | 0.00011082   | -8.42866e-05      |
| HC | 10 | 14 seconds | -0.000258528 | 0.000704061  | -0.000172469 | -0.000884672 | -0.000229728 | 0.000878828  | 0.000102747  | -0.000630718 | 0.000627632 | -0.000374336 | -2.37182e-05      |
| CLASSIC | 30 | 6 seconds | 0.000574382  | -0.000219898  | 0.00189601   | 0.000365038 | -0.00453484   | 0.000944871 | -0.00168528  | 0.00218952   | -0.00178567  | -0.000640037 | -0.000289591      |
|LSH|30| 13 seconds | -0.000146855  | -3.87748e-05  | 5.7024e-05  | -0.00109033  | -0.00050461  | -0.000987831 | -0.0017626  | 0.00321101   | -0.00122166  | 0.00215484   | -3.29796e-05      |
|HC|30| 21 seconds | 9.3524e-05    | 0.00125038   | 0.000960184  | -4.94717e-05  | 0.000746506  | 0.00181011   | -0.00179237  | -0.000577585  | -0.00257006  | 0.00305677   | 0.0002928         |

#### Συμπεράσματα

Τα αποτελέσματα της συνάρτησης στόχου silhouette που προβάλλουμε παραπάνω, είναι με βάσει την αναγωγή απτον latent στον initial (υπολογισμός μέσω της `_compute_latent_silhouettes()`) space.

Όπως αναμέναμε, οι χρόνοι φαίνονται μειωμένοι κατά αρκετό τόσο για το 10 όσο και για το 30 dimension. Αυτό που ίσως προκαλεί ευχάριστη έκπληξη είναι ότι το clustering δεν φαίνεται να χειροτερεύει και πάρα πολύ, παρά το ότι είναι πολύ πιο γρήγορο. Παρόλα αυτά, αυτό το αποτέλεσμα μπορεί να είναι και παραπλανητικό, αφού οι αλγόριθμοι μας ακόμα και στον αρχικό χώρο δεν μας δίνουν "καλά" αποτελέσματα, με το silhouette να παραμένει περίπου στο 0 αλλά να μην κάνει σημαντικά βήματα προς το 1. Άρα έχουμε "μέτρια αποτελέσματα" τα οποία ωστόσο παραμένουν ικανοποιητικά, και δεν χειροτερεύουν.

---

#### Compilation και εκτέλεση

Για compilation, απλά κάνουμε navigate στο project που θέλουμε και χρησιμοποιούμε το εργαλείο make εντός τους για να δημιουργήσουμε τα διάφορα εκτελέσιμα.


1) Για το compilation LSH: `make lsh`

2) Για το compilation Hypercube: `make hc`

3) Για το compilation Clustering: `make cluster`

Για τα διάφορα quick runs, μπορούμε να τρέξουμε τα επιμέρους για LSH/Hypercube/Cluster κάνοντας load τα διαφορετικά μας δεδομένα κάθε φορά

Για τα δεδομένα μεγέθους 10 χρησιμοποιείται για dataset το d10.bin και ως query set το q10.bin

Για τα δεδομένα μεγέθους 30 χρησιμοποιείται για dataset το d30_corr.bin και ως query set το q30_corr.bin

1) Για το compilation GNN/MRNG: `make graph_search`

Για quick runs:

1) Για το MRNG: `make run_mrng`

2) Για το GNN: `make run`

